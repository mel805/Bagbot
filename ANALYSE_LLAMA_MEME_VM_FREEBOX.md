# ğŸ” Analyse: Installer Llama sur la MÃŠME VM Freebox

**Date:** 24 DÃ©cembre 2025  
**Question rÃ©visÃ©e:** Est-il possible d'installer Llama sur la mÃªme VM Freebox que le bot Discord, mais avec des processus complÃ¨tement sÃ©parÃ©s (pas d'interaction) ?

---

## ğŸ“Š Ressources Actuelles de la Freebox

### MÃ©moire Disponible

```
RAM Physique:
  Total:        964 MB
  UtilisÃ©e:     403 MB
  Libre:        48 MB
  Disponible:   560 MB

SWAP:
  Total:        1.0 GB (1,048 MB)
  UtilisÃ©:      524 KB
  Disponible:   1,048 MB

TOTAL MÃ‰MOIRE VIRTUELLE:
  RAM + SWAP:   ~2,012 MB (2 GB)
```

### Consommation Actuelle

| Application | RAM | % |
|-------------|-----|---|
| Bot Discord | 156 MB | 15.7% |
| API Server | 97 MB | 9.8% |
| Dashboard | 66 MB | 6.7% |
| PM2 | 69 MB | 7.0% |
| SystÃ¨me | 15 MB | 1.5% |
| **TOTAL** | **403 MB** | **40.7%** |
| **Disponible** | **560 MB** | **59.3%** |

---

## ğŸ¤” FaisabilitÃ© Technique

### Option 1: ModÃ¨le Ultra-LÃ©ger (TinyLlama)

**TinyLlama 1.1B:**
- Taille du modÃ¨le: ~600 MB
- RAM nÃ©cessaire en fonctionnement: 800-1,000 MB
- RAM disponible actuellement: 560 MB

**Calcul avec TinyLlama:**
```
Ã‰tat actuel:
  BagBot + Services:    403 MB
  SystÃ¨me:              ~50 MB
  TinyLlama:            800 MB (minimum)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL BESOIN:       1,253 MB

Disponible:
  RAM:                  964 MB
  DÃ‰FICIT:             -289 MB

Avec SWAP (1 GB):
  RAM + SWAP:         2,012 MB
  MARGE:              +759 MB  âœ… Techniquement possible
```

**VERDICT: Techniquement POSSIBLE avec SWAP**

### âš ï¸ MAIS avec des ConsÃ©quences Importantes

#### 1. Performance Catastrophique

**Sans swap (impossible):**
- RAM insuffisante â†’ Crash immÃ©diat

**Avec swap (possible mais trÃ¨s lent):**
- ModÃ¨le chargÃ© partiellement en RAM, partiellement sur disque
- Chaque gÃ©nÃ©ration de texte = lecture/Ã©criture disque intensive
- **Performance attendue:** 0.5-2 tokens/seconde (vs 50-100 sur serveur dÃ©diÃ©)
- **Temps de rÃ©ponse:** 30-120 secondes pour 100 tokens
- **ExpÃ©rience utilisateur:** Inacceptable pour une app mobile

#### 2. Impact sur BagBot Discord

**ScÃ©nario d'utilisation simultanÃ©e:**

```
Utilisateur utilise l'app Android â†’ Llama gÃ©nÃ¨re du texte
   â†“
Llama lit/Ã©crit massivement sur le swap (disque)
   â†“
I/O disque saturÃ©e (SSD/eMMC limitÃ©)
   â†“
BagBot Discord ralenti (base de donnÃ©es, logs, etc.)
   â†“
Latence Discord augmente: 50ms â†’ 500ms+
   â†“
ExpÃ©rience utilisateurs Discord dÃ©gradÃ©e
```

**Impact estimÃ©:**
- Latence BagBot: +300-500ms pendant gÃ©nÃ©ration Llama
- Risque timeout Discord si gÃ©nÃ©ration longue
- Dashboard web ralenti
- Logs et backups ralentis

#### 3. StabilitÃ© du SystÃ¨me

**ProblÃ¨mes potentiels:**

1. **OOM Killer** (Out Of Memory Killer)
   - Si la RAM+SWAP est dÃ©passÃ©e, Linux tue des processus
   - Risque: BagBot ou Llama tuÃ© alÃ©atoirement

2. **Swap Thrashing**
   - Swap utilisÃ© massivement = usure du disque
   - Performance systÃ¨me globale trÃ¨s dÃ©gradÃ©e

3. **Blocages systÃ¨me**
   - I/O disque saturÃ©e = systÃ¨me figÃ© pendant 5-10 secondes
   - SSH peut devenir inaccessible temporairement

---

## âœ… Solutions Possibles sur la MÃŠME VM

### Solution A: TinyLlama avec Limitations Strictes (COMPROMIS)

**Installation:**
- ModÃ¨le: TinyLlama 1.1B (le plus petit possible)
- Backend: llama.cpp (plus Ã©conome qu'Ollama)
- Configuration: Mode "low memory" + quantization 4-bit
- Limite: 1 seule requÃªte Ã  la fois (file d'attente)

**Configuration technique:**
```bash
# Installer llama.cpp au lieu d'Ollama (plus lÃ©ger)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make -j2

# TÃ©lÃ©charger TinyLlama quantized (4-bit)
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# Serveur API avec limitations
./server \
  --model tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
  --ctx-size 512 \           # Contexte rÃ©duit
  --threads 1 \              # 1 seul thread (laisser 1 CPU pour BagBot)
  --parallel 1 \             # 1 seule requÃªte Ã  la fois
  --port 11434 \
  --host 0.0.0.0
```

**RÃ©sultats attendus:**

| MÃ©trique | Valeur |
|----------|--------|
| RAM utilisÃ©e | ~700-900 MB |
| SWAP utilisÃ©e | ~200-400 MB |
| Performance | 1-3 tokens/sec |
| Temps rÃ©ponse (100 tok) | 30-100 secondes |
| Impact BagBot | Moyen (+200ms latence) |
| QualitÃ© rÃ©ponses | â­â­ (correcte mais basique) |

**Avantages:**
- âœ… Sur la mÃªme VM (pas de serveur externe)
- âœ… Processus complÃ¨tement sÃ©parÃ©s
- âœ… CoÃ»t: 0â‚¬
- âœ… Pas de configuration rÃ©seau externe

**InconvÃ©nients:**
- âŒ Performance trÃ¨s mÃ©diocre (30-100 sec par rÃ©ponse)
- âŒ Impact sur BagBot (latence +200ms)
- âŒ QualitÃ© des rÃ©ponses limitÃ©e (petit modÃ¨le)
- âŒ Une seule requÃªte Ã  la fois
- âŒ ExpÃ©rience utilisateur dÃ©gradÃ©e

**Verdict:** **POSSIBLE TECHNIQUEMENT mais PEU RECOMMANDÃ‰**

---

### Solution B: API Cloud LÃ©gÃ¨re (HYBRIDE)

**Concept:** Garder tout sur la Freebox, mais utiliser une API cloud pour l'infÃ©rence

**Architecture:**
```
[Freebox VM]
  â”œâ”€ BagBot Discord (intact)
  â”œâ”€ API Server (intact)
  â”œâ”€ Dashboard (intact)
  â””â”€ Proxy API Llama (nouveau, ~20 MB RAM)
       â†“ (appelle via Internet)
[API Cloud Gratuite]
  â””â”€ Groq / OpenRouter / Together AI
       â””â”€ Llama 3.2 3B/8B (rapide)
```

**ImplÃ©mentation sur Freebox:**
```javascript
// /home/bagbot/Bag-bot/src/llama-proxy.js (nouveau fichier)
const express = require('express');
const axios = require('axios');

const app = express();
app.use(express.json());

const GROQ_API_KEY = process.env.GROQ_API_KEY; // API gratuite

app.post('/api/generate', async (req, res) => {
    try {
        const response = await axios.post('https://api.groq.com/openai/v1/chat/completions', {
            model: 'llama-3.2-3b-preview',
            messages: [{ role: 'user', content: req.body.prompt }],
        }, {
            headers: { 'Authorization': `Bearer ${GROQ_API_KEY}` }
        });
        
        res.json({ response: response.data.choices[0].message.content });
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});

app.listen(11434, '0.0.0.0');
```

**DÃ©marrer le proxy:**
```bash
# Sur Freebox
cd /home/bagbot/Bag-bot
GROQ_API_KEY="votre-clÃ©-gratuite" pm2 start src/llama-proxy.js --name llama-proxy
```

**RÃ©sultats attendus:**

| MÃ©trique | Valeur |
|----------|--------|
| RAM utilisÃ©e Freebox | +20 MB (proxy Node.js) |
| SWAP utilisÃ©e | 0 MB |
| Performance | 50-200 tokens/sec (API cloud) |
| Temps rÃ©ponse (100 tok) | 1-3 secondes |
| Impact BagBot | Aucun (<1ms) |
| QualitÃ© rÃ©ponses | â­â­â­â­â­ (Llama 3.2 3B/8B) |
| CoÃ»t | 0â‚¬ (Groq gratuit: 30 req/min) |

**Avantages:**
- âœ… RAM minimal sur Freebox (+20 MB seulement)
- âœ… Performance excellente (API cloud GPU)
- âœ… Aucun impact sur BagBot
- âœ… Processus sÃ©parÃ©s
- âœ… QualitÃ© maximale (grand modÃ¨le)
- âœ… Sur la mÃªme VM (le proxy)

**InconvÃ©nients:**
- âš ï¸ DÃ©pendance Ã  Internet
- âš ï¸ DÃ©pendance Ã  service externe (Groq)
- âš ï¸ Limite: 30 requÃªtes/minute (gratuit)

**Verdict:** **RECOMMANDÃ‰** - Meilleur compromis

---

### Solution C: GPT4All (Alternative Plus LÃ©gÃ¨re)

**GPT4All** est une alternative Ã  Ollama, plus Ã©conome en mÃ©moire.

**ModÃ¨les compatibles:**
- GPT4All Mini: ~400 MB RAM
- Performance: ~2-5 tokens/sec
- QualitÃ©: Correcte pour usage basique

**Installation:**
```bash
# Sur Freebox
pip install gpt4all

# Serveur Python simple
python3 << 'EOF'
from gpt4all import GPT4All
from flask import Flask, request, jsonify

app = Flask(__name__)
model = GPT4All("orca-mini-3b.ggmlv3.q4_0.bin")  # ~400 MB

@app.route('/api/generate', methods=['POST'])
def generate():
    prompt = request.json['prompt']
    response = model.generate(prompt, max_tokens=100)
    return jsonify({'response': response})

app.run(host='0.0.0.0', port=11434)
EOF
```

**RÃ©sultats attendus:**

| MÃ©trique | Valeur |
|----------|--------|
| RAM utilisÃ©e | ~500-600 MB |
| SWAP utilisÃ©e | ~100-200 MB |
| Performance | 2-5 tokens/sec |
| Temps rÃ©ponse (100 tok) | 20-50 secondes |
| Impact BagBot | Faible (+50-100ms) |
| QualitÃ© rÃ©ponses | â­â­â­ (correcte) |

**Verdict:** **COMPROMIS ACCEPTABLE**

---

## ğŸ“Š Tableau Comparatif des Solutions

| Solution | RAM | Impact BagBot | Performance | QualitÃ© | DifficultÃ© | RecommandÃ© |
|----------|-----|---------------|-------------|---------|------------|------------|
| **A. TinyLlama local** | 700 MB | âŒ Moyen | âŒ TrÃ¨s lent | â­â­ | â­â­â­ | âŒ Non |
| **B. Proxy API Cloud** | 20 MB | âœ… Aucun | âœ… Excellent | â­â­â­â­â­ | â­ | âœ…âœ…âœ… Oui |
| **C. GPT4All** | 500 MB | âš ï¸ Faible | âš ï¸ Moyen | â­â­â­ | â­â­ | âœ… Oui |
| **D. Oracle Cloud** | 0 MB | âœ… Aucun | âœ… Excellent | â­â­â­â­â­ | â­â­ | âœ…âœ… Oui |

---

## ğŸ¯ Ma Recommandation Finale

### Pour la MÃŠME VM Freebox: Solution B (Proxy API Cloud)

**Pourquoi:**
1. âœ… **RAM minimale** (+20 MB vs +700 MB)
2. âœ… **Aucun impact** sur BagBot Discord
3. âœ… **Performance excellente** (API cloud GPU)
4. âœ… **QualitÃ© maximale** (Llama 3.2 3B/8B)
5. âœ… **Sur la mÃªme VM** (le proxy tourne sur Freebox)
6. âœ… **Gratuit** (Groq Free Tier: 30 req/min)

**Architecture:**
```
Application Android
       â†“ HTTP
Freebox:11434 (Proxy Node.js - 20 MB RAM)
       â†“ API
Groq Cloud (Llama 3.2 3B - GPU rapide)
       â†“
RÃ©ponse en 1-3 secondes
```

**Code complet prÃªt Ã  dÃ©ployer ci-dessous** â†“

---

## ğŸ’» Installation Solution B (Proxy API - RECOMMANDÃ‰)

### Ã‰tape 1: CrÃ©er compte Groq (gratuit)

1. Aller sur: https://console.groq.com
2. S'inscrire (gratuit)
3. CrÃ©er une API Key
4. Noter la clÃ©: `gsk_...`

### Ã‰tape 2: CrÃ©er le proxy sur Freebox

```bash
# Se connecter Ã  la Freebox
ssh -p 33000 bagbot@88.174.155.230

# CrÃ©er le fichier proxy
cat > /home/bagbot/Bag-bot/src/llama-proxy.js << 'EOF'
const express = require('express');
const axios = require('axios');

const app = express();
app.use(express.json());

const GROQ_API_KEY = process.env.GROQ_API_KEY || 'VOTRE-CLE-ICI';
const GROQ_API_URL = 'https://api.groq.com/openai/v1/chat/completions';

// API compatible Ollama
app.post('/api/generate', async (req, res) => {
    try {
        console.log(`[Llama Proxy] RequÃªte: ${req.body.prompt?.substring(0, 50)}...`);
        
        const response = await axios.post(GROQ_API_URL, {
            model: 'llama-3.2-3b-preview',
            messages: [{
                role: 'user',
                content: req.body.prompt
            }],
            temperature: req.body.temperature || 0.7,
            max_tokens: req.body.max_tokens || 512,
        }, {
            headers: {
                'Authorization': `Bearer ${GROQ_API_KEY}`,
                'Content-Type': 'application/json'
            }
        });
        
        const text = response.data.choices[0].message.content;
        console.log(`[Llama Proxy] RÃ©ponse: ${text.substring(0, 50)}...`);
        
        // Format compatible Ollama
        res.json({
            model: 'llama3.2:3b',
            response: text,
            done: true
        });
    } catch (error) {
        console.error('[Llama Proxy] Erreur:', error.message);
        res.status(500).json({ error: error.message });
    }
});

// API chat (optionnel)
app.post('/api/chat', async (req, res) => {
    try {
        const response = await axios.post(GROQ_API_URL, {
            model: 'llama-3.2-3b-preview',
            messages: req.body.messages,
        }, {
            headers: { 'Authorization': `Bearer ${GROQ_API_KEY}` }
        });
        
        res.json({
            message: response.data.choices[0].message,
            done: true
        });
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});

// Health check
app.get('/api/tags', (req, res) => {
    res.json({
        models: [{
            name: 'llama3.2:3b',
            size: 2000000000,
            modified_at: new Date().toISOString()
        }]
    });
});

const PORT = 11434;
app.listen(PORT, '0.0.0.0', () => {
    console.log(`[Llama Proxy] DÃ©marrÃ© sur port ${PORT}`);
    console.log(`[Llama Proxy] Utilisant Groq API (Llama 3.2 3B)`);
});
EOF
```

### Ã‰tape 3: DÃ©marrer le proxy

```bash
cd /home/bagbot/Bag-bot

# Option 1: Avec variable d'environnement
GROQ_API_KEY="gsk_VOTRE_CLE_ICI" pm2 start src/llama-proxy.js --name llama-proxy

# Option 2: Ã‰diter le fichier pour mettre la clÃ© directement
nano src/llama-proxy.js  # Remplacer VOTRE-CLE-ICI
pm2 start src/llama-proxy.js --name llama-proxy

# Sauvegarder la config PM2
pm2 save
```

### Ã‰tape 4: Tester

```bash
# Test depuis Freebox
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.2:3b","prompt":"Bonjour!","stream":false}'

# Test depuis Internet (votre PC/tÃ©lÃ©phone)
curl -X POST http://88.174.155.230:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.2:3b","prompt":"Bonjour!","stream":false}'
```

### Ã‰tape 5: Utiliser depuis Android

**Le code reste identique** Ã  `EXEMPLE_ANDROID_LLAMA.kt`:
```kotlin
val llama = OllamaClient("http://88.174.155.230:11434")
llama.generate("Question") { response ->
    println("Llama: $response")
}
```

---

## âœ… RÃ©sultat Final

**Avec Solution B (Proxy API):**

```
Freebox VM (88.174.155.230):
â”œâ”€ BagBot Discord:    156 MB  âœ… Intact
â”œâ”€ API Server:        97 MB   âœ… Intact
â”œâ”€ Dashboard:         66 MB   âœ… Intact
â”œâ”€ PM2:               69 MB   âœ… Intact
â””â”€ Llama Proxy:       +20 MB  âœ… Nouveau (Node.js)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   TOTAL:             408 MB  âœ… OK (556 MB libres)

Performance:
  Latence:            1-3 secondes
  QualitÃ©:            Excellente (Llama 3.2 3B)
  Impact BagBot:      0% (aucun)
  CoÃ»t:               0â‚¬
```

**Tout sur la mÃªme VM, processus complÃ¨tement sÃ©parÃ©s, impact minimal!**

---

## ğŸ“ RÃ©ponse Ã  Votre Question

### â“ Est-ce possible d'installer Llama sur la mÃªme VM Freebox ?

**OUI, avec 3 options:**

| Option | Faisable | Impact RAM | Impact BagBot | Performance | RecommandÃ© |
|--------|----------|------------|---------------|-------------|------------|
| **TinyLlama local** | âœ… Oui | +700 MB | âŒ Moyen | âŒ TrÃ¨s lent | âŒ Non |
| **Proxy API Cloud** | âœ… Oui | +20 MB | âœ… Aucun | âœ… Excellent | âœ…âœ…âœ… Oui |
| **GPT4All** | âœ… Oui | +500 MB | âš ï¸ Faible | âš ï¸ Moyen | âœ… Acceptable |

### ğŸ¯ Ma Recommandation

**Solution B: Proxy API Cloud (Groq)**

**Pourquoi:**
- âœ… Sur la mÃªme VM Freebox (processus proxy)
- âœ… RAM minimale (+20 MB seulement)
- âœ… Aucun impact sur BagBot
- âœ… Performance excellente (1-3 sec)
- âœ… QualitÃ© maximale (Llama 3.2 3B)
- âœ… Gratuit (Groq Free Tier)
- âœ… Code fourni et prÃªt Ã  dÃ©ployer

**Installation:** 10 minutes (voir ci-dessus)

---

**Voulez-vous que je vous aide Ã  installer le proxy sur votre Freebox maintenant ?**
